{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial\n",
    "import tensorflow as tf \n",
    "import sklearn.naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_csv('/Users/juliachen/Desktop/DSCI552/hw/hw5/data/ps5_tweets_text.csv')\n",
    "train_label = pd.read_csv('/Users/juliachen/Desktop/DSCI552/hw/hw5/data/ps5_tweets_labels_as_numbers.csv')\n",
    "\n",
    "test_tweets = pd.read_csv('/Users/juliachen/Desktop/DSCI552/hw/hw5/data/ps5_tweets_text_for_the_kaggle_competition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (37041, 2)\n",
      "Test data shape (3798, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data shape {train_tweets.shape}\")\n",
    "print(f\"Test data shape {test_tweets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''delete id column'''\n",
    "train_tweets.drop(\"Id\",axis=1,inplace=True)\n",
    "train_label.drop(\"Id\",axis=1,inplace=True)\n",
    "#test_tweets.drop(\"Id\",axis=1,inplace=True)\n",
    "\n",
    "'''Combine train_tweets and train_label into train data set'''\n",
    "dataset = pd.concat([train_label, train_tweets], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    10282\n",
       "1     8930\n",
       "2     6930\n",
       "4     5953\n",
       "0     4946\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extremely Negative (0), Negative (1), Neutral (2), Positive (3), Extremely Positive (4)\n",
    "Therefore, we can see the there the most large tweets are extremely postive, only few are extremely negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29632,)\n",
      "(7409,)\n",
      "(29632, 5)\n",
      "(7409, 5)\n"
     ]
    }
   ],
   "source": [
    "'''Split into training and validation set'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                  test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "y_val_use_for_metric = y_val\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_val = pd.get_dummies(y_val)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of tokenization is to represent words in a way that computer can understand and be able to train it. \n",
    "\n",
    "num_words: the maximum number of words to keep, based on word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>',\n",
    "                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                     lower=True)\n",
    "\n",
    "#training_sequences = tokenizer.fit_on_sequences(x_train)\n",
    "#valid_sequences = tokenizer.fit_on_sequences(x_val)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "word_ind=tokenizer.word_index\n",
    "\n",
    "training_sentences = tokenizer.texts_to_sequences(x_train)\n",
    "valid_sentences = tokenizer.texts_to_sequences(x_val)\n",
    "test_sentences = tokenizer.texts_to_sequences(test_tweets['Tweet'])\n",
    "\n",
    "#\"\"\"Pad and trunckate the squence \"\"\"\n",
    "traning_padded = pad_sequences(training_sentences)\n",
    "valid_padded = pad_sequences(valid_sentences)\n",
    "test_padded =  pad_sequences(test_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 85        \n",
      "=================================================================\n",
      "Total params: 160,357\n",
      "Trainable params: 160,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim=16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding\"),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29632, 70)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traning_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29632, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traning_padded = np.asarray(traning_padded).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "926/926 [==============================] - 3s 2ms/step - loss: 1.5607 - accuracy: 0.2827 - val_loss: 1.4310 - val_accuracy: 0.3766\n",
      "Epoch 2/10\n",
      "926/926 [==============================] - 2s 2ms/step - loss: 1.3392 - accuracy: 0.4396 - val_loss: 1.1531 - val_accuracy: 0.5344\n",
      "Epoch 3/10\n",
      "926/926 [==============================] - 2s 2ms/step - loss: 1.0614 - accuracy: 0.5855 - val_loss: 1.0161 - val_accuracy: 0.6114\n",
      "Epoch 4/10\n",
      "926/926 [==============================] - 2s 2ms/step - loss: 0.8918 - accuracy: 0.6722 - val_loss: 0.9444 - val_accuracy: 0.6475\n",
      "Epoch 5/10\n",
      "926/926 [==============================] - 2s 3ms/step - loss: 0.7992 - accuracy: 0.7183 - val_loss: 0.9117 - val_accuracy: 0.6670\n",
      "Epoch 6/10\n",
      "926/926 [==============================] - 2s 2ms/step - loss: 0.7281 - accuracy: 0.7520 - val_loss: 0.9146 - val_accuracy: 0.6746\n",
      "Epoch 7/10\n",
      "926/926 [==============================] - 2s 2ms/step - loss: 0.6866 - accuracy: 0.7714 - val_loss: 0.9076 - val_accuracy: 0.6869\n",
      "Epoch 8/10\n",
      "926/926 [==============================] - 1s 2ms/step - loss: 0.6491 - accuracy: 0.7841 - val_loss: 0.9084 - val_accuracy: 0.6921\n",
      "Epoch 9/10\n",
      "926/926 [==============================] - 1s 2ms/step - loss: 0.6297 - accuracy: 0.7931 - val_loss: 0.9162 - val_accuracy: 0.6927\n",
      "Epoch 10/10\n",
      "926/926 [==============================] - 1s 2ms/step - loss: 0.6147 - accuracy: 0.8033 - val_loss: 0.9413 - val_accuracy: 0.6908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faec19116d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(traning_padded, y_train, epochs=10, validation_data = (valid_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.690781481981374\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.78      0.71       818\n",
      "           1       0.64      0.64      0.64      1779\n",
      "           2       0.70      0.74      0.72      1350\n",
      "           3       0.68      0.64      0.66      2165\n",
      "           4       0.80      0.74      0.77      1297\n",
      "\n",
      "    accuracy                           0.69      7409\n",
      "   macro avg       0.69      0.71      0.70      7409\n",
      "weighted avg       0.69      0.69      0.69      7409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "pred_val1 = model.predict(valid_padded)\n",
    "pred_val1 = np.argmax(pred_val1, axis=1)\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(pred_val1, y_val_use_for_metric))\n",
    "print(metrics.classification_report(pred_val1, y_val_use_for_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 score:  0.690781481981374\n",
      "Macro F1 score:  0.6994808001859135\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"Micro F1 score: \",f1_score(pred_val1, y_val_use_for_metric, average='micro'))\n",
    "print(\"Macro F1 score: \",f1_score(pred_val1, y_val_use_for_metric, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1 = model.predict(test_padded)\n",
    "pred_test1 = np.argmax(pred_test1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Result: 0.64091\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({'Id': range(0,3798), 'Predicted': pred_test1})\n",
    "# df.to_csv('/Users/juliachen/Desktop/rnn1', index=False)\n",
    "print(\"Kaggle Result:\", 0.64091)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          640000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 677,509\n",
      "Trainable params: 677,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim=64\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding\"),\n",
    "    #tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.LSTM(embedding_dim),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.summary()\n",
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "926/926 [==============================] - 28s 28ms/step - loss: 1.3913 - accuracy: 0.3750 - val_loss: 0.9212 - val_accuracy: 0.6670\n",
      "Epoch 2/10\n",
      "926/926 [==============================] - 28s 30ms/step - loss: 0.7259 - accuracy: 0.7443 - val_loss: 0.7730 - val_accuracy: 0.7340\n",
      "Epoch 3/10\n",
      "926/926 [==============================] - 27s 29ms/step - loss: 0.6049 - accuracy: 0.7986 - val_loss: 0.7442 - val_accuracy: 0.7476\n",
      "Epoch 4/10\n",
      "926/926 [==============================] - 28s 30ms/step - loss: 0.5274 - accuracy: 0.8285 - val_loss: 0.7328 - val_accuracy: 0.7469\n",
      "Epoch 5/10\n",
      "926/926 [==============================] - 28s 31ms/step - loss: 0.4593 - accuracy: 0.8478 - val_loss: 0.7412 - val_accuracy: 0.7456\n",
      "Epoch 6/10\n",
      "926/926 [==============================] - 26s 28ms/step - loss: 0.3949 - accuracy: 0.8707 - val_loss: 0.8088 - val_accuracy: 0.7414\n",
      "Epoch 7/10\n",
      "926/926 [==============================] - 28s 30ms/step - loss: 0.3456 - accuracy: 0.8866 - val_loss: 0.8486 - val_accuracy: 0.7326\n",
      "Epoch 8/10\n",
      "926/926 [==============================] - 29s 32ms/step - loss: 0.3108 - accuracy: 0.8946 - val_loss: 0.9132 - val_accuracy: 0.7326\n",
      "Epoch 9/10\n",
      "926/926 [==============================] - 30s 33ms/step - loss: 0.2708 - accuracy: 0.9114 - val_loss: 1.0290 - val_accuracy: 0.7178\n",
      "Epoch 10/10\n",
      "926/926 [==============================] - 27s 29ms/step - loss: 0.2250 - accuracy: 0.9246 - val_loss: 1.0597 - val_accuracy: 0.7233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7faec2c8c040>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(traning_padded, y_train, epochs=10, validation_data = (valid_padded, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7233094884599811\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.72      0.72       976\n",
      "           1       0.68      0.68      0.68      1754\n",
      "           2       0.78      0.81      0.80      1373\n",
      "           3       0.70      0.68      0.69      2095\n",
      "           4       0.76      0.75      0.75      1211\n",
      "\n",
      "    accuracy                           0.72      7409\n",
      "   macro avg       0.73      0.73      0.73      7409\n",
      "weighted avg       0.72      0.72      0.72      7409\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_val2 = model2.predict(valid_padded)\n",
    "pred_val2 = np.argmax(pred_val2, axis=1)\n",
    "\n",
    "print('Accuracy: ',metrics.accuracy_score(pred_val2, y_val_use_for_metric))\n",
    "print(metrics.classification_report(pred_val2, y_val_use_for_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 score:  0.7233094884599811\n",
      "Macro F1 score:  0.729318420998671\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"Micro F1 score: \",f1_score(pred_val2, y_val_use_for_metric, average='micro'))\n",
    "print(\"Macro F1 score: \",f1_score(pred_val2, y_val_use_for_metric, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test2 = model.predict(test_padded)\n",
    "pred_test2 = np.argmax(pred_test2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle Result: 0.64091\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.DataFrame.from_dict({'Id': range(0,3798), 'Predicted': pred_test2})\n",
    "df.to_csv('/Users/juliachen/Desktop/rnn2', index=False)\n",
    "print(\"Kaggle Result:\", 0.64091)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
